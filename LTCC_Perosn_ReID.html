<html>

<head>
    <link rel="StyleSheet" href="style.css" type="text/css" media="all">
    <title>LTCC</title>
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body>
    <br>
    <div class="center-div" align="center">
        <span style="font-size:28px">Long-Term Cloth-Changing Person Re-identification</span>
    </div> 

    <br>
    <table align="center" width="1200px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://naiq.github.io/" target="_blank">Xuelin Qian</a></span>
                    </div>
                </td>
				
				<td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://wxwangiris.github.io/" target="_blank">Wenxuan Wang</a></span>
                    </div>
                </td>
				
				<td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="http://www.robots.ox.ac.uk/~lz/" target="_blank">Li Zhang</a></span>
                    </div>
                </td>
				
				<td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://fangruizhu.github.io/" target="_blank">Fangrui Zhu</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="http://yanweifu.github.io/index.html" target="_blank">Yanwei Fu</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html" target="_blank">Tao Xiang</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="http://www.yugangjiang.info/" target="_blank">Yu-Gang Jiang</a></span>
                    </div>
                </td>

				<td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://scholar.google.com/citations?user=DTbhX6oAAAAJ&hl=en" target="_blank">Xiangyang Xue</a></span>
                    </div>
                </td>

            </tr>
        </tbody>
    </table>
    <br>
    <table align="center" width="700px">
        <tbody>
            <tr>
                <td align="center" width="100px">
                    <div class="center-div">
                        <span style="font-size:18px">Fudan University &nbsp; &nbsp; University of Oxford &nbsp; &nbsp; University of Surrey</span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
    <br>




    <hr>
    <div class="center-div" align="center">
        <h2>Abstract</h2>
    </div>
    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> Person re-identification (Re-ID) aims to match a target person across camera views at different locations and times. 
			Existing Re-ID studies focus on the short-term cloth-consistent setting, under which a person re-appears in different camera views with the same 
			outfit. A discriminative feature representation learned by existing deep Re-ID models is thus dominated by the visual appearance of clothing. In 
			this work, we focus on a much more difficult yet practical setting where person matching is conducted over long-duration, e.g., over days and months 
			and therefore inevitably under the new challenge of changing clothes. This problem, termed Long-Term Cloth-Changing (LTCC) Re-ID is much understudied 
			due to the lack of large scale datasets. The first contribution of this work is a new LTCC dataset containing people captured over a long period of 
			time with frequent clothing changes. As a second contribution, we propose a novel Re-ID method specifically designed to address the cloth-changing 
			challenge. Specifically, we consider that under cloth-changes, soft-biometrics such as body shape would be more reliable. We, therefore, introduce 
			a shape embedding module as well as a cloth-elimination shape-distillation module aiming to eliminate the now unreliable clothing appearance features 
			and focus on the body shape information. Extensive experiments show that superior performance is achieved by the proposed model on the new LTCC dataset.
            </div>
        </td>
       
    </table>
    <br><br>
	
	
	
	<hr>
    <div class="center-div" align="center">
        <h2>Motivation</h2>
    </div>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/arxiv2020_LTCC_intro.png" width="600px"></a><br>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
	<br>
	
	<table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> Illustration of the long-term cloth-changing Re-ID task and dataset. The task is to match the same person under 
			cloth-changes from different views, and the dataset contains same identities with diverse clothes.
            </div>
        </td>
    </table>
    <br><br>
	
	
	
	<hr> 
	<div class="center-div" align="center">
        <h2>Long-Term Cloth-Changing (LTCC) Dataset</h2>
    </div>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> To facilitate the study of Long-Term Cloth-Changing (LTCC) Re-ID, we collect a new LTCC person Re-ID dataset. LTCC contains
			17,119 person images of 152 identities, and each identity is captured by at least two cameras. To further explore the cloth-changing Re-ID scenario, we
			assume that different people will not wear identical outfits (however visually similar they may be), and annotate each image with a cloth label as well.
			Note that the changes in hairstyle or carrying items, e.g., hat, bag, or laptop, do not affect the cloth label. Finally, dependent on whether there 
			is a cloth-change, the dataset can be divided into two subsets: one cloth-change set where 91 persons appearing with 416 different sets of outfits in 
			14,783 images, and one cloth-consistent subset containing the remaining 61 identities with 2,336 images without outfit changes. On average, there are 5 
			different clothes for each cloth-changing person, with the number of outfit changes ranging from 2 to 14.
            </div>
        </td>
    </table>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/arxiv2020_LTCC/arxiv2020_LTCC_dataset-2.png" width="600px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>LTCC Dataset</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table>
	
	<br>
	<table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> Examples of some people wearing the same and different clothes in LTCC dataset. There exist various illuminations, 
			occlusion, camera view, carrying, and pose changes. This dataset has already been released.
			<!--<a href="">[Dataset Google Drive]</a>
            <a href="">[Dataset Baidu Drive](password:)</a>-->
            </div>
        </td>
    </table>
	<br><br>
	
	
		<hr> 
	<div class="center-div" align="center">
        <h2>Dataset Download</h2>
    </div>
	
	<table align="center" width="1000px">
        <td>
            <div>
            <b><span style="font-size:16pt"> Data Release Agreement</span> </b>
			<br><br>
			<span style="font-size:14pt"> Dataset records are made available to researchers only after the receipt and acceptance of a completed 
			and signed Database Release Agreement.</span>
			<br><br>
			<span style="font-size:14pt"> <a href="images/arxiv2020_LTCC/Restrictions for The Use of LTCC Dataset.pdf">[Data Release Protocol]</a> </span>
			<br><br>
			<span style="font-size:14pt"> Please submit requests for the dataset unless otherwise indicated: 
			xuelinq92@gmail.com or xlqian@nwpu.edu.cn <del>xlqian15@fudan.edu.cn<del></span>	
			<br><br>
			<b><span style="font-size:16pt"> Directory Structure</span> </b>
			<br><br>
			<span style="font-size:14pt"> The directory structure is similar to Market-1501. Specially, the package contains four folders:<br/>
			(1) "train". There are 9,576 images with 77 identities in this folder used for training (46 cloth-change IDs + 31 cloth-consistent IDs).<br/>
			(2) "test". There are 7,050 images with 75 identities in this folder used for testing (45 cloth-change IDs + 30 cloth-consistent IDs).<br/>
			(3) "query". There are 493 images with 75 identities. We randomly select one query image for each camera and each outfit.<br/>
			(4) "info". There are 4 TXT files depicting the details of the identity split in the train and test folder, which can be used for different experiment settings (e.g., training with cloth-changing images only).</span>
			<br><br>
			<span style="font-size:14pt">Naming Rule: <br/>
			Take "103_1_c9_017008.png" for example, the first part of '103' means the person identity (there are totally 152 identities, annotated from 
			000 to 151). The second '1' denotes the clothes counter, i.e., the first suit of the person '103'. "c9" means the ninth camera (there are 
			totally 12 cameras). The last term of '017008' is just the image counter, we don't use it during training and testing.</span>
			<br><br>
			<b><span style="font-size:16pt"> Evaluation Protocol</span> </b>
			<br><br>
			<span style="font-size:14pt"> During inference, we load all gallery images but set remove or junk index to filter the images that violate the evaluation protocol. Below, we use pseudocode to explain in more detail the two settings used in the paper.<br/>
			(1) "Standard Setting": Gallery images with the same identity as the query person are filtered.<br/>
			<table align="center">
				<tbody>
				    <tr>
					<td>
					    <div class="center-div" align="center">
						<a href="#"><img src="images/arxiv2020_LTCC/standard_code.jpg" width="600px"></a><br>
					    </div>
					</td>
				    </tr>
				</tbody>
			</table>
			(2) "Cloth-changing Setting": Gallery images with the same identity or the same clothes as the query person are filtered.</span>
				
			
            </div>
        </td>
    </table>
	
    <br><br>
	
	

    <hr>
    <div class="center-div" align="center">
        <h2>Cloth-Elimination Shape-Distillation (CESD) Module</h2>
    </div>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> With cloth-changing now commonplace in LTCC Re-ID, existing Re-ID models are expected to struggle because they assume 
			that the clothing appearance is consistent and relies on clothing features to distinguish people from each other. Our key idea is to remove the 
			cloth-appearance related information completely and only focus on view/pose-change-insensitive body shape information. To this end, we introduce a 
			Shape Embedding (SE) to help shape feature extraction and a Cloth-Elimination Shape-Distillation (CESD) module to eliminate cloth-related information. 
            </div>
        </td>
    </table>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/arxiv2020_LTCC/arxiv2020_LTCC_framwork.png" width="700px"></a><br>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
	<br>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> Illustration of our framework and the details of Cloth-Elimination ShapeDistillation (CESD) module. Here, we introduce 
			Shape Embedding (SE) module to extract structural features from human keypoints, followed by learning identity-sensitive and cloth-insensitive 
			representations using the CESD module.
            </div>
        </td>
    </table>
    <br><br>
	

	
	<hr> 
	<div class="center-div" align="center">
        <h2>Results</h2>
    </div>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/arxiv2020_LTCC/arxiv2020_LTCC_results-1.png" width="500px"></a><br>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
	<br><br>

	<br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/arxiv2020_LTCC/arxiv2020_LTCC_results-2.png" width="500px"></a><br>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
	<br><br>
	


    <hr>
    <div class="center-div" align="center">
        <h2 id="paper">Paper</h2>
    </div>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <img class="layered-paper-big" style="height:200px" src="images/arxiv2020_LTCC/arxiv2020_LTCC_page.png">
                </td>
                <td>
			<b><span style="display:inline-block;width:600px;font-size:14pt"> Long-Term Cloth-Changing Person Re-identification </b>
                    <br><br>
                    <span style="font-size:14pt"> <me>Xuelin Qian</me>, Wenxuan Wang, Li Zhang, Fangrui Zhu, Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue.</span>
                    <br><br>
                    <span style="font-size:18px">
                        <a href="https://arxiv.org/abs/2005.12633">[Paper]</a>
                        <a href="images\arxiv2020_LTCC\BibTex.txt">[Bibtex]</a>
			<a href="https://drive.google.com/file/d/1N6yLX1zn_flibDxrbPWpc1jC9yuBBNoi/view?usp=sharing">[Code]</a>
                        <!--<a href="">[GitHub]</a>
						<a href="">[Dataset Google Drive]</a>
						<a href="">[Dataset Baidu Drive](password:)</a>-->
                    </span>
					
                </td>
            </tr>
        </tbody>
    </table>
	
	<br><br>


    
    <hr>
    <table align="center" width="980px">
        <tbody>
            <tr>
                <td>
                    <left>
                        <div class="center-div" align="center">
                            <h2>Acknowledgements</h2>
                        </div>
			<!--<div class="center-div" align="center">
			<a href='https://clustrmaps.com/site/1b8uf'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=357ec8&w=a&t=tt&d=L8KPRjwz8BaK3RDCjPXAkfjrb6Js-hoXgAEny-sdLrc&co=e1e6e9&ct=fdfdfd' width="250" height="135" align="center"/></a>
			</div>-->
			<p><center>
	  	        <div id="clustrmaps-widget" style="width:10%">
	  		<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=L8KPRjwz8BaK3RDCjPXAkfjrb6Js-hoXgAEny-sdLrc"></script>
	                </div>
			</center></p>
			<div class="center-div" align="center"> 
                            The website is modified from this <a href="https://www.cs.cmu.edu/~wyuan1/pcn/">template</a>.
                        </div>
                    </left>
                </td>
            </tr>
        </tbody>
    </table>
    <br>



</body>

</html>
