<html>

<head>
    <link rel="StyleSheet" href="style.css" type="text/css" media="all">
    <title>MuDeep</title>
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body>
    <br>
    <div class="center-div" align="center">
        <span style="font-size:28px">Leader-Based Multi-Scale Attention Deep Architecture for Person Re-Identification</span>
    </div>

    <br>
    <table align="center" width="1000px">
        <tbody>
            <tr>
                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://naiq.github.io/" target="_blank">Xuelin Qian</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="http://yanweifu.github.io/index.html" target="_blank">Yanwei Fu</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html" target="_blank">Xiang Tao</a></span>
                    </div>
                </td>

                <td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="http://www.yugangjiang.info/" target="_blank">Yu-Gang Jiang</a></span>
                    </div>
                </td>

				<td align="center" width="150px">
                    <div class="center-div">
                        <span style="font-size:18px"><a href="https://scholar.google.com/citations?user=DTbhX6oAAAAJ&hl=en" target="_blank">Xiangyang Xue</a></span>
                    </div>
                </td>

            </tr>
        </tbody>
    </table>
    <br>
    <table align="center" width="700px">
        <tbody>
            <tr>
                <td align="center" width="100px">
                    <div class="center-div">
                        <span style="font-size:18px">Fudan University, China &nbsp; &nbsp; University of Surrey, United Kingdom</span>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>

    <br>

    <hr>
    <div class="center-div" align="center">
        <h2>Abstract</h2>
    </div>
    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> Person re-identification (re-id) aims to match people across non-overlapping camera views in a public space. 
			This is a challenging problem because the people captured in surveillance videos often wear similar clothing. Consequently, the differences 
			in their appearance are typically subtle and only detectable at particular locations and scales. In this paper, we propose a deep re-id 
			network (MuDeep) that is composed of two novel types of layers – a multi-scale deep learning layer, and a leader-based attention learning 
			layer. Specifically, the former learns deep discriminative feature representations at different scales, while the latter utilizes the 
			information from multiple scales to lead and determine the optimal weightings for each scale. The importance of different spatial locations 
			for extracting discriminative features is learned explicitly via our leader-based attention learning layer. Extensive experiments 
			are carried out to demonstrate that the proposed MuDeep outperforms the state-of-the-art on a number of benchmarks and has a better 
			generalization ability under a domain generalization setting.
            </div>
        </td>
       
    </table>
    <br><br>

    <hr>
    <div class="center-div" align="center">
        <h2>MuDeep</h2>
    </div>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> A novel multi-scale representation learning architecture is proposed for learning discriminative person 
			appearance features at multiple spatial scales and locations. Critically, the multiple scales refer to different resolution levels of 
			filters, rather than multiscale inputs. And a leader-based attention learning layer can utilize the information computed at all scales 
			to lead, and dynamically determines the important spatial locations in the feature extraction at each scale.
            </div>
        </td>
    </table>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/PAMI19_Framework.png" width="700px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>MuDeep Framework</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table>
	<br>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> The multi-scale stream layer first analyzes feature maps with multiple scales. Then the leader-based
			attention learning layer is followed to automatically discover and emphasize important spatial locations. Finally, the global and 
			local branch layer is utilized to extract discriminate features from global and local parts. Note that the parameters of each 
			scale are not shared. ‘LAL’ means the Leader-based Attention Learning layer.
            </div>
        </td>
    </table>
    <br><br>

	
	<hr> 
	<div class="center-div" align="center">
        <h2>Extended Makeup Face Dataset (EMFD)</h2>
    </div>

    <table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> To facilitate this study, we assembled a new makeup face verfication database of 1102 face images 
			that is 551 pairs of individuals. Each pair has one makeup and one non-makeup face images of the same individual. 
			All face images are collected from the Internet with text information about makeup or non-makeup. So the 
			labels of makeup and non-makeup, and the facial identities are collected together with the face images from 
			the Internet. This dataset have the facial images of large areas of acne, glasses occlusion, head posture changes and
			so on. Some face examples from our database are shown in below.
            </div>
        </td>
    </table>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/cvpr2020_makeup/cvpr2020_expand_dataset.png" width="600px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>Extended Makeup Face Dataset</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table>
	
	<br>
	<table align="center" width="1000px">
        <td>
            <div>
            <span style="font-size:18px"> The file name in the folder is xxx_m or xxx_n, where xxx indicates the xxxth person, 
			"m" indicates the image with makeup, and "n" indicates the image without makeup. 
			This dataset is for non-commercial reseach purposes (such as academic research) only.
			<a href="https://drive.google.com/open?id=1wpqZCYh7gGKz897C-hQbgR7CHbJdBGxg">[Dataset Google Drive]</a>
            <a href="https://pan.baidu.com/s/1jhBqJeXShYyChfp1K3RcCw">[Dataset Baidu Drive](password:n2zw)</a>			
            </div>
        </td>
    </table>
	<br><br>

    <hr>
    <div class="center-div" align="center">
        <h2 id="paper">Paper and Data</h2>
    </div>
    <table align="center">

        <tbody>
            <tr>
                <td>
                    <img class="layered-paper-big" style="height:200px" src="images/cvpr2020_makeup/cvpr20_page.png">
                </td>
                <td>
					<b><span style="display:inline-block;width:600px;font-size:14pt">  FMMu-Net: Face Morphological Multi-branch Network for Makeup-invariant Face Verification.
                    </span></b>
                    <br><br>
                    <span style="font-size:14pt">  Wenxuan Wang<sup>*</sup>, Yanwei Fu<sup>*</sup>, Xuelin Qian, Yu-Gang Jiang, Qi Tian, Xiangyang Xue</span>
                    <br><br>
                    <span style="font-size:14px">
                        <a href="">[Paper]</a>
                        <a href="">[Bibtex]</a>
                        <!--<a href="">[GitHub]</a>-->
                        <a href="https://drive.google.com/open?id=1wpqZCYh7gGKz897C-hQbgR7CHbJdBGxg">[Dataset Google Drive]</a>
                        <a href="https://pan.baidu.com/s/1jhBqJeXShYyChfp1K3RcCw">[Dataset Baidu Drive](password:n2zw)</a>
                    </span>
                </td>
            </tr>
        </tbody>
    </table>
    <br><br>
	
	
	<hr> 
	<div class="center-div" align="center">
        <h2>Results</h2>
    </div>

    <br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/cvpr2020_makeup/cvpr20_results-1.png" width="500px"></a><br>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>
	<br><br>

	<br>
    <table align="center">
        <tbody>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <a href="#"><img src="images/cvpr2020_makeup/cvpr2020_results-2.png" width="700px"></a><br>
                    </div>
                </td>
            </tr>
            <tr>
                <td>
                    <div class="center-div" align="center">
                        <span style="font-size:16px"><i>Generated Images</i>
                        </span></div>
                </td>
            </tr>
        </tbody>
    </table>
	<br><br>
    
    <hr>
    <table align="center" width="980px">
        <tbody>
            <tr>
                <td>
                    <left>
                        <div class="center-div" align="center">
                            <h2>Acknowledgements</h2>
                        </div>
                        <div class="center-div" align="center"> 
                            The website is modified from this <a href="https://www.cs.cmu.edu/~wyuan1/pcn/">template</a>.
                        </div>
                    </left>
                </td>
            </tr>
        </tbody>
    </table>
    <br>



</body>

</html>
